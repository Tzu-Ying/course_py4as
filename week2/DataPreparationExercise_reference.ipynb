{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Exercise Reference\n",
    "------\n",
    "## 基本作業\n",
    "\n",
    "請讀取104年花東空品區三個測站的資料，進行分析，並回答以下問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們從檔案讀取資料，檢查一下資料的維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hualian = pd.read_excel('../data/104年花蓮站_20160320.xls')\n",
    "hualian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taidong = pd.read_excel('../data/104年臺東站_20160320.xls')\n",
    "taidong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guanshan = pd.read_excel('../data/104年關山站_20160323.xls')\n",
    "guanshan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "關山站的資料筆數明顯比另外兩個站少，讓我們檢查一下個測項的資料天數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(guanshan['測項'],'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6205 == (365 * 3) + (5 * 3) + 5095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "關山站少了三個測項，另外有三個測項少了5天的資料，如果補上這些資料，的確就跟花蓮、台東站一樣了。\n",
    "\n",
    "## 資料清理\n",
    "\n",
    "接下來，我們要把遺失值換成 np.nan，然後把雨量的 NR 換成 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_epa_nan(x):\n",
    "    ''' Search for missing value symbol and assign np.nan '''\n",
    "    if re.findall('\\#|\\*|x', str(x))!=[]:\n",
    "        return(np.nan)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def detect_epa_norain(x):\n",
    "    ''' Replace 'NR' (no-rain) with 0 '''\n",
    "    if str(x)=='NR':\n",
    "        return(0)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def clean_epa_station(x):\n",
    "    ''' Clean up a EPA station dataset '''\n",
    "    # Rename columns\n",
    "    col_names = ['date','station','item','h00','h01','h02','h03','h04','h05','h06','h07','h08','h09',\n",
    "                'h10','h11','h12','h13','h14','h15','h16','h17','h18','h19','h20','h21','h22','h23']\n",
    "    x.columns = col_names\n",
    "    # Process NA and NR\n",
    "    floatdata = x.iloc[:,3:]\n",
    "    floatdata = floatdata.applymap(detect_epa_nan)\n",
    "    floatdata = floatdata.applymap(detect_epa_norain)\n",
    "    floatdata.astype(np.float32)\n",
    "    x.iloc[:,3:] = floatdata\n",
    "    # Done\n",
    "    return(x)\n",
    "\n",
    "hualian = clean_epa_station(hualian)\n",
    "taidong = clean_epa_station(taidong)\n",
    "guanshan = clean_epa_station(guanshan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guanshan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料格式轉換\n",
    "\n",
    "如果我們想看各個測項單獨的狀態，以及彼此之間的關係，最佳的資料呈現方式，是將每個測項轉換成單獨的時間序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve one item from EPA data and form a time series\n",
    "def retrieve_epa_item(data, var):\n",
    "    tmp = data.loc[data['item']==var,:]\n",
    "    ts = pd.melt(tmp, id_vars=['date'], value_vars=tmp.keys()[3:], var_name='hour', value_name=var)\n",
    "    ts[var] = ts[var].astype(np.float32)\n",
    "    return(ts)\n",
    "\n",
    "# Convert EPA dataset to a collection of time-series of items\n",
    "def convert_epa_itemts(data):\n",
    "    # All items\n",
    "    items = list(set(data['item']))\n",
    "    # Create the 1st dataframe\n",
    "    newdata = retrieve_epa_item(data, items[0])\n",
    "    # Loop through the rest of items\n",
    "    for i in items[1:]:\n",
    "        tmp = retrieve_epa_item(data, i)\n",
    "        newdata = newdata.merge(tmp, on=['date','hour'], how='left')\n",
    "    # Sort with date-hour to make the time-series in order\n",
    "    newdata = newdata.sort_values(['date', 'hour'])\n",
    "    # Done\n",
    "    return(newdata)\n",
    "\n",
    "# Do the conversion\n",
    "hl = convert_epa_itemts(hualian)\n",
    "td = convert_epa_itemts(taidong)\n",
    "gs = convert_epa_itemts(guanshan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此一來，我們可以很快的計算某個單獨測項有多少個遺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(td) - td['O3'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們也可以用 [`DataFrame.apply`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) 一次計算每個測項的遺失值數量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.apply(lambda x: len(x)-x.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三個測站的測項不完全相同\n",
    "print(hl.keys())\n",
    "print(td.keys())\n",
    "print(gs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轉換成新格式後，我們可以利用 pandas.DataFrame.corr() 快速的計算同個測站不同測項的相關矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.iloc[:,2:].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以計算不同測站品項之間的相關性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = pd.DataFrame({'hualian':hl['PM2.5'], 'taidong':td['PM2.5'], 'guanshan':gs['PM2.5']})\n",
    "pm25.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把單一測項轉換成時間序列之後，也可以利用 `pandas.DataFrame.interpolate()` 做內插來取代遺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td['NOx'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdo3_int = td['NOx'].interpolate()\n",
    "tdo3_int.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在新的資料格式裡，我們也可以用 pandas.pivot_table() 來看測項之間的關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = td.loc[:,['PM2.5','RH','WIND_SPEED']]\n",
    "tmp['RH'] = pd.cut(tmp['RH'], bins=[0., 25., 50., 75., 100.])\n",
    "tmp['WIND_SPEED'] = pd.qcut(tmp['WIND_SPEED'], 4)\n",
    "#tmp.head()\n",
    "tmp.pivot_table(columns=['RH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 額外挑戰\n",
    "\n",
    "## 在目錄下尋找特定檔案\n",
    "\n",
    "前面的示範，都是處理少數檔案的情況，但是環保署的全部資料包含76個測站，雖然 copy-and-paste 76次也可以解決問題，但是 python 提供了更方便的工具 `os.walk()`，讓我們可以「遊走」指定資料夾底下的子目錄，然後用 `str.endswith()` 來尋找所有的 `.xls` 檔案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_xls_files(path):\n",
    "    urls = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            if(fname.endswith(\".xls\")):\n",
    "                urls.append(os.path.join(root, fname))\n",
    "    return(urls)\n",
    "\n",
    "epafiles = find_xls_files('../data/104_HOUR_00_20160323/')\n",
    "epafiles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以透過這個方法，把所有測站的資料都一次做處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data from all files\n",
    "data = []\n",
    "nancounts = []\n",
    "for f in epafiles:\n",
    "    tmp = pd.read_excel(f)\n",
    "    tmp = clean_epa_station(tmp)\n",
    "    tmp = convert_epa_itemts(tmp)\n",
    "    nancounts.append(tmp.apply(lambda x: len(x)-x.count()))\n",
    "    data.append(tmp)\n",
    "\n",
    "nancounts = pd.concat(nancounts, axis=1).T\n",
    "nancounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後我們可以很快的計算每個測站、每個測項遺失值的數量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nancounts['station'] =(epafiles)\n",
    "nancounts.set_index('station', inplace=True)\n",
    "nan_items = nancounts.apply(lambda x: len(x)-x.count())\n",
    "nan_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_stations = nancounts.T.apply(lambda x: len(x)-x.count())\n",
    "nan_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_stations.loc[nan_stations>6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過缺失值特徵的分析，我們可以思考如何建置比較長期的資料庫：我們希望有盡可能多的測站和測項，以及盡可能長的資料時間，透過合理的工具將資料補齊，以進一步位我們好奇的科學問題做出解答。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
