{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principle Component Analysis Using Python\n",
    "\n",
    "\n",
    "\n",
    "[Course Module: Applied Data Analysis for Atmospheric Sciences Using Python]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "November 2018\n",
    "Ting-Shuo Yo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "- What is PCA?\n",
    "- Some Simple Examples\n",
    "- Perform PCA with Python\n",
    "  - [MNIST dataset](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 主成分分析 (Principle Component Analysis, PCA)\n",
    "\n",
    "- 主成分分析是很常用的一種探索式資料分析(exploratory analysis)工具，這個數學工具協助我們計算**資料本身的變異**，找出資料本身變化的主要特徵，也就是**主成分（principle component, PC）**，讓我們可以很快的檢視資料裡相對較為特殊的狀態（pattern），並且達到減少資料集維度的效果。\n",
    "\n",
    "<img src='figures/GaussianScatterPCA.png' align='center' width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA is more than 100 years old\n",
    "\n",
    "- 主成分分析是 1901 由 Karl Pearson 所提出，用於分析數據及建立數理模型。\n",
    "- 其方法主要是通過對共變異數矩陣進行特徵分解（eigen decomposition），以得出數據的主成分（即特徵向量）與它們的權值（即特徵值）。\n",
    "- PCA是以特徵量分析多元統計分布的方法中最基本的一種。\n",
    "- 其結果可以理解為對原數據中的變異數做出解釋：哪一個方向上的數據值對變異數的影響最大？\n",
    "\n",
    "<img src='figures/Karl_Pearson_1912.jpg' align='right' height='220' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematics of PCA\n",
    "\n",
    "- 我們總共有 $N$ 筆觀測資料，$X = \\{x_i\\}_{i=1,..,N} = \\{x_1, x_2, ..., x_N\\}$\n",
    "- 我們的每個觀測包含了 $M$ 個變量，所以每個觀測可以用 $x_i = \\{x_{i1}, x_{i2}, ..., x_{iM}\\}$ 來表示\n",
    "- 如此， $X = \\{ x_{ij}\\}_{i=1,..,N, j=1,...,M}$\n",
    "- 我們希望找到一組 $Y = \\{Y_1, Y_2, ..., Y_M\\}$，滿足以下條件：\n",
    "  - $Y = \\Sigma_{i=1}^{M} a_i * X_i$\n",
    "  - $Y$ 依照可以解釋 $X$ 變異量的大小排序，如此，我們可以用少數 $K$ 個 $Y$，來代表大部分 $X$ 的變化。\n",
    "- 在數學上，我們可以證明 $X$ 的共變數矩陣的 eigen vectors 滿足以上的條件。 \n",
    "- And we will stop right here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 100 多年來的補充\n",
    "\n",
    "- From eigen decomposition to Singular Value Decomposition (SVD)\n",
    "  - To overcome unsolvable data matrix\n",
    "- How to handle very big N: \n",
    "  - incremental PCA (online PCA)\n",
    "- How to handle very big M: \n",
    "  - randomized PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Very Simple Example\n",
    "\n",
    "讓我們來看看 PCA 的實際操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "首先，我們製作一個 6\\*2 的矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a 6x2 matrix as an example\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "我們可以從資料的散佈圖看得出來，這是一個很接近一條斜線的分佈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick scatter plot\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "讓我們用這組資料來進行 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA objetc with 2 PCs\n",
    "pca = PCA(n_components=2)\n",
    "# Use the prepraed data for calculation\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "這樣就做完了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "讓我們來看看計算的結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportion of variance explained by each PC\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eigen values\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PCs, i.e., eigen vectors\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "只看數字，似乎不太容易理解發生了什麼事情，讓我們把這些數字畫出來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do scatter plot\n",
    "plt.plot(X[:,0], X[:,1],'o')\n",
    "# Add PC1 as blue solid line\n",
    "plt.plot(pca.components_[0][0]*np.array([3,-3]),pca.components_[0][1]*np.array([3,-3]),'k-', lw=2)\n",
    "# Add PC2 as red dashed line\n",
    "plt.plot(pca.components_[1][0]*np.array([2,-2]),pca.components_[1][1]*np.array([2,-2]),'r--', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A More Complicated Example\n",
    "\n",
    "剛剛的例子，是把二維的資料降低成一維，我們再來看看另一組三維資料的例子（來源：[scikit-learn](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_3d.html#sphx-glr-auto-examples-decomposition-plot-pca-3d-py)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Gael Varoquaux\n",
    "#          Jaques Grobler\n",
    "#          Kevin Hughes\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Create the data\n",
    "\n",
    "e = np.exp(1)\n",
    "np.random.seed(4)\n",
    "\n",
    "\n",
    "def pdf(x):\n",
    "    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)\n",
    "                  + stats.norm(scale=4 / e).pdf(x))\n",
    "\n",
    "y = np.random.normal(scale=0.5, size=(30000))\n",
    "x = np.random.normal(scale=0.5, size=(30000))\n",
    "z = np.random.normal(scale=0.1, size=len(x))\n",
    "\n",
    "density = pdf(x) * pdf(y)\n",
    "pdf_z = pdf(5 * z)\n",
    "\n",
    "density *= pdf_z\n",
    "\n",
    "a = x + y\n",
    "b = 2 * y\n",
    "c = a - b + z\n",
    "\n",
    "norm = np.sqrt(a.var() + b.var())\n",
    "a /= norm\n",
    "b /= norm\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot the figures\n",
    "def plot_figs(fig_num, elev, azim):\n",
    "    fig = plt.figure(fig_num, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)\n",
    "\n",
    "    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)\n",
    "    Y = np.c_[a, b, c]\n",
    "\n",
    "    # Using SciPy's SVD, this would be:\n",
    "    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(Y)\n",
    "    pca_score = pca.explained_variance_ratio_\n",
    "    V = pca.components_\n",
    "\n",
    "    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T\n",
    "    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]\n",
    "    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]\n",
    "    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]\n",
    "    x_pca_plane.shape = (2, 2)\n",
    "    y_pca_plane.shape = (2, 2)\n",
    "    z_pca_plane.shape = (2, 2)\n",
    "    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "\n",
    "elev = -40\n",
    "azim = -80\n",
    "plot_figs(1, elev, azim)\n",
    "\n",
    "elev = 30\n",
    "azim = 20\n",
    "plot_figs(2, elev, azim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Real World Example: 花蓮測站的 PM2.5\n",
    "\n",
    "我們在資料清理的課程中，使用了環保署測站的資料作為示範，我們接下來用相同的資料來示範 PCA 的使用。\n",
    "\n",
    "首先，我們要讀進原始資料，然後加以清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def detect_epa_nan(x):\n",
    "    ''' Search for missing value symbol and assign np.nan '''\n",
    "    if re.findall('\\#|\\*|x', str(x))!=[]:\n",
    "        return(np.nan)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def detect_epa_norain(x):\n",
    "    ''' Replace 'NR' (no-rain) with 0 '''\n",
    "    if str(x)=='NR':\n",
    "        return(0)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def clean_epa_station(x):\n",
    "    ''' Clean up a EPA station dataset '''\n",
    "    # Rename columns\n",
    "    col_names = ['date','station','item','h00','h01','h02','h03','h04','h05','h06','h07','h08','h09',\n",
    "                'h10','h11','h12','h13','h14','h15','h16','h17','h18','h19','h20','h21','h22','h23']\n",
    "    x.columns = col_names\n",
    "    # Process NA and NR\n",
    "    floatdata = x.iloc[:,3:]\n",
    "    floatdata = floatdata.applymap(detect_epa_nan)\n",
    "    floatdata = floatdata.applymap(detect_epa_norain)\n",
    "    floatdata.astype(np.float32)\n",
    "    x.iloc[:,3:] = floatdata\n",
    "    # Done\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Real World Example: 花蓮測站的 PM2.5\n",
    "\n",
    "我們在資料清理的課程中，使用了環保署測站的資料作為示範，我們接下來用相同的資料來示範 PCA 的使用。\n",
    "\n",
    "首先，我們要讀進原始資料，然後加以清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data from file\n",
    "hualian = pd.read_excel('../data/104年花蓮站_20160320.xls')\n",
    "# Clean up the raw data\n",
    "hualian = clean_epa_station(hualian)\n",
    "# Select pm2.5\n",
    "data = hualian[hualian['item']=='PM2.5']\n",
    "data.reset_index(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Does a PC Mean?\n",
    "\n",
    "在開始進行主成分分析之前，我們先想一下，目前我們的資料格式是什麼意思？假設我們忽略前三個欄位（日期、測站、測項），後面的24個欄位即是我們的 ${X_i}$，是一天內每個小時的 PM2.5濃度，而每一個觀測是一天內的情況。假設我們直接拿這個資料集去做 PCA，我們找到的每個**新座標軸**，都會是一日內PM2.5濃度逐時的變化型態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先處理遺失值\n",
    "data_dropna = data.dropna()\n",
    "# 製作一個 complete transformation 的 PCA\n",
    "pca_pm25 = PCA(n_components=24)\n",
    "pca_pm25.fit(data_dropna.iloc[:,4:])\n",
    "# 先來看看前三個主成分（PC1, PC2, PC3）長什麼樣子\n",
    "plt.plot(pca_pm25.components_[0], label='PC1')\n",
    "plt.plot(pca_pm25.components_[1], 'r--', label='PC2')\n",
    "plt.plot(pca_pm25.components_[2], 'g-.', label='PC3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Many PCs Do We Need?\n",
    "\n",
    "把原本的24個座標軸，旋轉到新的24個座標軸，是完整的（complete）線性轉換，但是主成分分析的主要目的之一，就是希望能不需要用到24個座標軸。那麼，我們該取前幾個主成分來用呢？\n",
    "\n",
    "由於主成分是一找「能解釋原有變異的大小」來排序的，我們可以透過觀察每個主成分的「可解釋變異量」（[`sklearn.decomposition.PCA.explained_variance_ratio_`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)）和「累積可解釋變異量」，來決定最後取用的主成分個數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.cumsum(X) will calculate the cumulative sum of X\n",
    "plt.bar(range(24),pca_pm25.explained_variance_ratio_)\n",
    "plt.plot(np.cumsum(pca_pm25.explained_variance_ratio_), 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Few Rules about the Number of PCs to Use\n",
    "\n",
    "關於如何選擇恰當的主成分個數 $K$，並沒有標準答案，但一般來說有幾個來自於經驗的建議：\n",
    "\n",
    "1. 「累積可解釋變異量」需要達到一個百分比，依應用領域和實際數據的不同，一般可能會是 60% ~ 90%\n",
    "2. 個別主成分的「可解釋變異量」較前一個主成分明顯的降低，或是本身低到一個閾值以下\n",
    "3. 你堅定的相信 $K$ 應該是某個數字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ratio of Variance Explained by PCs:')\n",
    "print(np.round(pca_pm25.explained_variance_ratio_, 6))\n",
    "print()\n",
    "print('Accumulative Ratio of Variance Explained by PCs:')\n",
    "print(np.round(np.cumsum(pca_pm25.explained_variance_ratio_), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以這個例子來說，$K$ = 2, 3, 5, 8，甚至 9, 16 都算是合理的選擇。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to Use the First $K$ PCs\n",
    "\n",
    "假設我們選擇 $K$ = 3，接下來我們會做兩件事情：\n",
    "\n",
    "1. 觀察與詮釋每個主成分的意義\n",
    "2. 觀察與詮釋每個觀測在個別主成分上的投影\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前三個主成分（PC1, PC2, PC3）\n",
    "plt.plot(pca_pm25.components_[0], label='PC1')\n",
    "plt.plot(pca_pm25.components_[1], 'r--', label='PC2')\n",
    "plt.plot(pca_pm25.components_[2], 'g-.', label='PC3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[`sklearn.decomposition.PCA.fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)可以幫我們把資料投影到新的座標軸上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = pca_pm25.fit_transform(data_dropna.iloc[:,4:])\n",
    "plt.plot(proj[:,0], label='PC1')\n",
    "plt.plot(proj[:,1], 'r--', label='PC2')\n",
    "plt.plot(proj[:,2], 'g-.', label='PC3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從這個例子裡，我們可以看到，直接使用 dropna()，會使投影後的資料不足365天，所以這個情況下，或許用某種內插來補足遺失值會是比較好的選擇。\n",
    "\n",
    "此外，我們示範的資料，每個欄位是一個小時的觀測值，如果用上段課程的資料整理技巧，把欄位換成不同測站同時間的 PM2.5 觀測值，那麼計算出來的主成分代表的意義就會是「空間分布」而不是一日內逐時的變化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "在這個範例中，我們簡短的介紹了主成分分析（Principle Component Analysis, PCA），並以幾個例子示範了 PCA 的用法。以下是幾點摘要：\n",
    "\n",
    "- PCA 主要可以用來降低資料的維度：用較少的座標軸來詮釋大多數的資料變異\n",
    "- PCA 的使用方式：\n",
    "  - 決定 $K$\n",
    "  - 詮釋主成分的意義\n",
    "  - 詮釋投影的意義\n",
    "- PCA 也可以視為一種「濾波」（filtering）的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
