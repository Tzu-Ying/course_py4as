{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環保署各測站的 PM2.5 資料\n",
    "\n",
    "這個 jupyter notebook 提供了工具和範例，把環保署測站資料的 PM2.5 集合在一起，每個欄位是一個測站的時間序列。在使用之前，請先下載 2015 年環保署所有測站資料，並解壓縮放在 `../data/104_HOUR_00_20160323` 中。\n",
    "\n",
    "首先，我們先載入需要用到的函式庫，以及在「資料清理」課程中使用到的一些函數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "\n",
    "def detect_epa_nan(x):\n",
    "    ''' Search for missing value symbol and assign np.nan '''\n",
    "    if re.findall('\\#|\\*|x', str(x))!=[]:\n",
    "        return(np.nan)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def detect_epa_norain(x):\n",
    "    ''' Replace 'NR' (no-rain) with 0 '''\n",
    "    if str(x)=='NR':\n",
    "        return(0)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def clean_epa_station(x):\n",
    "    ''' Clean up a EPA station dataset '''\n",
    "    # Rename columns\n",
    "    col_names = ['date','station','item','h00','h01','h02','h03','h04','h05','h06','h07','h08','h09',\n",
    "                'h10','h11','h12','h13','h14','h15','h16','h17','h18','h19','h20','h21','h22','h23']\n",
    "    x.columns = col_names\n",
    "    # Process NA and NR\n",
    "    floatdata = x.iloc[:,3:]\n",
    "    floatdata = floatdata.applymap(detect_epa_nan)\n",
    "    floatdata = floatdata.applymap(detect_epa_norain)\n",
    "    floatdata.astype(np.float32)\n",
    "    x.iloc[:,3:] = floatdata\n",
    "    # Done\n",
    "    return(x)\n",
    "\n",
    "# Retrieve one item from EPA data and form a time series\n",
    "def retrieve_epa_item(data, var):\n",
    "    tmp = data.loc[data['item']==var,:]\n",
    "    ts = pd.melt(tmp, id_vars=['date'], value_vars=tmp.keys()[3:], var_name='hour', value_name=var)\n",
    "    ts[var] = ts[var].astype(np.float32)\n",
    "    return(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尋找目錄下的所有檔案\n",
    "\n",
    "由於環保署資料包含了很多個檔案，又依照空品區的劃分，放在不同的資料夾底下，因此我們需要借助 python 的 [`os.walk()`](https://docs.python.org/3/library/os.html) 函數，幫我們自動「走遍」資料夾底下的每個檔案。\n",
    "\n",
    "此外，由於資料的檔名包含了測站名稱，我們可以順便把測站名稱取出來，作為欄位的名稱，所以我們需要用到 [`str.find()`](https://www.tutorialspoint.com/python/string_find.htm) 函數，來幫我們找到測站名稱在檔名裡的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through the specified path and find all files ended with 'xls'\n",
    "def find_xls_files(path):\n",
    "    ids = []    # 測站名稱\n",
    "    urls = []   # 檔案完整路徑\n",
    "    for root, dirs, files in os.walk(path):             # os.walk() 會傳回「根目錄」、「路徑」、和「檔名」三個字串 list\n",
    "        for fname in files:                             # 每個檔名\n",
    "            if(fname.endswith(\".xls\")):                 # 如果是以 xls 結尾\n",
    "                fid = fname[(fname.find('104年')+4):][:(fname.find('_2016')-5)]\n",
    "                ids.append(fid)\n",
    "                urls.append(os.path.join(root, fname))\n",
    "    return((ids,urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於環保署資料的檔名是以 *YYY年XX站_YYYYMMDD.xls* 的格式命名 ，例如：104年花蓮站_20160320.xls，我們要取出字串裡的 *XX*，所以我們要：\n",
    "\n",
    "1. 從完整檔名裡找到 *'104年'* 之後的字串 A\n",
    "2. 從 A 裡找到 *'_2016'* 前面的字串，不包含「站」\n",
    "\n",
    "因此，我們的寫法會是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花蓮站_20160320.xls\n",
      "花蓮\n"
     ]
    }
   ],
   "source": [
    "fn = '104年花蓮站_20160320.xls'\n",
    "fn1 = fn[(fn.find('104年')+4):]\n",
    "print(fn1)\n",
    "fn2 = fn1[:(fn1.find('_2016')-1)]\n",
    "print(fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['二林', '南投', '埔里', '大里', '彰化', '忠明', '沙鹿', '竹山', '線西', '西屯', '豐原', '三重', '中壢', '中山', '古亭', '土城', '基隆', '士林', '大同', '大園', '平鎮', '新店', '新莊', '松山', '板橋', '林口', '桃園', '永和', '汐止', '淡水', '菜寮', '萬華', '萬里', '觀音', '陽明', '龍潭', '冬山', '宜蘭', '三義', '新竹', '湖口', '竹東', '苗栗', '頭份', '臺東', '花蓮', '關山', '金門', '馬公', '馬祖', '善化', '嘉義', '安南', '崙背', '斗六', '新港', '新營', '朴子', '臺南', '臺西', '麥寮', '仁武', '前金', '前鎮', '大寮', '小港', '屏東', '左營', '復興', '恆春', '林園', '楠梓', '橋頭', '潮州', '美濃', '鳳山']\n"
     ]
    }
   ],
   "source": [
    "ids, epafiles = find_xls_files('../data/104_HOUR_00_20160323/')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實際進行資料的讀取、清理與合併\n",
    "\n",
    "接下來，我們就用上面的函數，讀進各個測站的資料，清理之後，抽出 PM2.5 的資料，然後用日期跟時間的欄位加以合併。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>二林</th>\n",
       "      <th>南投</th>\n",
       "      <th>埔里</th>\n",
       "      <th>大里</th>\n",
       "      <th>彰化</th>\n",
       "      <th>忠明</th>\n",
       "      <th>沙鹿</th>\n",
       "      <th>竹山</th>\n",
       "      <th>...</th>\n",
       "      <th>屏東</th>\n",
       "      <th>左營</th>\n",
       "      <th>復興</th>\n",
       "      <th>恆春</th>\n",
       "      <th>林園</th>\n",
       "      <th>楠梓</th>\n",
       "      <th>橋頭</th>\n",
       "      <th>潮州</th>\n",
       "      <th>美濃</th>\n",
       "      <th>鳳山</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/01/01</td>\n",
       "      <td>h00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>2015/01/01</td>\n",
       "      <td>h01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>2015/01/01</td>\n",
       "      <td>h02</td>\n",
       "      <td>41.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>2015/01/01</td>\n",
       "      <td>h03</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2015/01/01</td>\n",
       "      <td>h04</td>\n",
       "      <td>69.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date hour    二林    南投    埔里    大里    彰化    忠明    沙鹿    竹山  ...   \\\n",
       "0     2015/01/01  h00  29.0  48.0  55.0  53.0  36.0  44.0  15.0  51.0  ...    \n",
       "365   2015/01/01  h01  30.0  44.0  51.0  55.0  39.0  37.0  21.0  48.0  ...    \n",
       "730   2015/01/01  h02  41.0  39.0  47.0  58.0  36.0  38.0  37.0  46.0  ...    \n",
       "1095  2015/01/01  h03  46.0  37.0  40.0  53.0  33.0  22.0  56.0  40.0  ...    \n",
       "1460  2015/01/01  h04  69.0  34.0  32.0  43.0  36.0  29.0  67.0  34.0  ...    \n",
       "\n",
       "        屏東    左營    復興    恆春    林園    楠梓    橋頭    潮州    美濃    鳳山  \n",
       "0     77.0  77.0  86.0  21.0  71.0  76.0  83.0  61.0  37.0  72.0  \n",
       "365   64.0  86.0  74.0  17.0  77.0  74.0  67.0  69.0  39.0  62.0  \n",
       "730   61.0  83.0  58.0  19.0  63.0  78.0  66.0  71.0  39.0  51.0  \n",
       "1095  46.0  71.0  54.0  15.0  59.0  66.0  56.0  65.0  36.0  37.0  \n",
       "1460  43.0  66.0  57.0  21.0  56.0  46.0  54.0  47.0  37.0  44.0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect data from all files\n",
    "f = epafiles[0]                                         # Start from the first file\n",
    "tmp = pd.read_excel(f)                                  # Read in file\n",
    "tmp = clean_epa_station(tmp)                            # Clean up nan and no-rain\n",
    "data = retrieve_epa_item(tmp, 'PM2.5')                  # Retrieve PM2.5\n",
    "nancounts = []\n",
    "for f in epafiles[1:]:\n",
    "    tmp = pd.read_excel(f)                                  # Read in file\n",
    "    tmp = clean_epa_station(tmp)                            # Clean up nan and no-rain\n",
    "    tmp = retrieve_epa_item(tmp, 'PM2.5')                   # Retrieve PM2.5\n",
    "    data = data.merge(tmp, on=['date','hour'], how='left')  # Aggregate data\n",
    "    nancounts.append(tmp.apply(lambda x: len(x)-x.count())) # Count the number of nan\n",
    "    #print(f + \": \" + str(len(tmp)))\n",
    "\n",
    "data = data.sort_values(['date', 'hour'])                   # Sort data\n",
    "cnames = list(data.columns[:2]) + ids\n",
    "data.columns = cnames\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料的輸出\n",
    "\n",
    "Python 的輸出/輸入非常多樣，我們這裡介紹 pandas.DataFrame 內建的輸出/輸入工具。在我們讀取資料的時候，使用了 [pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table) 與 [pandas.read_excel()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html)，想當然耳，pandas 也提供了輸出成這兩種格式的工具 [pandas.DataFrame.to_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) / [pandas.DataFrame.to_excel()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html)。\n",
    "\n",
    "對於「表格式」的資料來說，CSV 大概是最常用的資料格式，下面我們就把整理好的資料輸出成 csv 檔，方便後續的分析來使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/pm25_2015.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
